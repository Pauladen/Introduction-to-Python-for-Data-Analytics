{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc5SsPwDAiJx"
   },
   "source": [
    "# Data cleaning and preprocessing\n",
    "## Data Normalization and Scaling\n",
    "In data preprocessing, one essential step is data normalization and scaling. These techniques help us to standardize the range of independent variables or features of data. We'll delve into the importance of data normalization and scaling, common techniques, and their implementation in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP0xGXJiA77R"
   },
   "source": [
    "## Understanding the Importance of Data Normalization and Scaling\n",
    "\n",
    "Machine learning algorithms perform better when input numerical variables fall within a similar scale. Without normalization or scaling, features with higher values may dominate the model's outcome. This could lead to misleading results and a model that fails to capture the  influence of other features.\n",
    "\n",
    "Normalization and scaling bring different features to the same scale, allowing a fair comparison and ensuring that no particular feature dominates others. Moreover, these techniques can also accelerate the training process. For instance, gradient descent converges faster when features are on similar scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWknD_aEBJjV"
   },
   "source": [
    "### Techniques for Data Normalization\n",
    "\n",
    "Data normalization is a method to change the values of numeric columns in a dataset to a common scale. Here are a few normalization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41RUx3rhBrhr"
   },
   "source": [
    "**Min-Max Scaling**\n",
    "\n",
    "Min-max scaling is one of the simplest methods to normalize data. It scales and translates each feature individually such that it is in the range of 0 to 1.\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Create a simple dataset\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "# Create a scaler, fit and transform the data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eMdkIYSMAQeA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Create a simple dataset\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a scaler, fit and transform the data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Create a simple dataset\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "# Create a scaler, fit and transform the data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHERZt55BgNt"
   },
   "source": [
    "**Z-score Normalization (Standardization)**\n",
    "\n",
    "This technique standardizes the feature such that it has a mean of 0 and a standard deviation\n",
    "of 1. It redistributes the features with their mean at 0 and standard deviation as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Utf07ryB2Nb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a scaler, fit and transform the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qim8FqEhB6dn"
   },
   "source": [
    "**Feature Scaling Techniques**\n",
    "\n",
    "Feature scaling is an umbrella term for techniques that change the range of a feature. In\n",
    "addition to the aforementioned normalization techniques, the following methods are also used\n",
    "for feature scaling:\n",
    "\n",
    "* **Robust Scaling**\n",
    "\n",
    "Robust scaling is similar to min-max scaling but uses the interquartile range instead of the\n",
    "min-max, making it robust to outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKQ0Z0hUCMIg"
   },
   "outputs": [],
   "source": [
    "## Robust Scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# Create a scaler, fit and transform the data\n",
    "scaler = RobustScaler()\n",
    "robust_scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8iEw3vkAUWg"
   },
   "source": [
    "### Implementing Data Normalization and Scaling with Python\n",
    "\n",
    "Let's take a closer look at how to implement normalization and scaling with a Python example:\n",
    "\n",
    "This script below creates a simple dataframe with three columns. We then initialize three different\n",
    "types of scalers - `MinMaxScaler`, `StandardScaler`, and `RobustScaler`. We use these scalers to fit\n",
    "and transform our dataframe, creating three new dataframes each scaled by a different\n",
    "method. Finally, we print the original data and the transformed data to see the differences.\n",
    "\n",
    "Data normalization and scaling are powerful techniques that can help to prepare your data for machine learning algorithms. These techniques ensure that all features contribute equally to the final decision of the model, regardless of their original scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Q6slq7bCdzH"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Let's create a simple dataframe\n",
    "df = pd.DataFrame({\n",
    "'A': [1, 2, 3, 4, 5],\n",
    "'B': [100, 200, 400, 800, 1000],\n",
    "'C': [200, 400, 600, 800, 1000]\n",
    "})\n",
    "\n",
    "# Initialize a min-max scaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the dataframe\n",
    "df_min_max = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Initialize a standard scaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Scale the dataframe\n",
    "df_std = pd.DataFrame(std_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Initialize a robust scaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Scale the dataframe\n",
    "df_robust = pd.DataFrame(robust_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"Original Data\")\n",
    "print(df)\n",
    "print(\"\\nMin-Max Scaled Data\")\n",
    "print(df_min_max)\n",
    "print(\"\\nStandard Scaled Data\")\n",
    "print(df_std)\n",
    "print(\"\\nRobust Scaled Data\")\n",
    "print(df_robust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytf5DWRtDGpp"
   },
   "source": [
    "## Feature Selection and Extraction\n",
    "\n",
    "Feature selection and extraction are pivotal steps in the data preprocessing pipeline for machine learning and data science projects. These techniques can make the difference between a model that performs exceptionally well and one that falls flat. In this chapter, we will cover the basics of feature selection and extraction, discuss some common techniques, and\n",
    "implement these techniques in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAMdV_9-DaTD"
   },
   "source": [
    "### Introduction to Feature Selection and Extraction\n",
    "\n",
    "Feature selection and extraction techniques are used to reduce the dimensionality of the data,\n",
    "thus enhancing computational efficiency and potentially improving the model's performance.\n",
    "\n",
    "* **Feature Selection**\n",
    "Feature selection is the process of selecting a subset of relevant features (variables or predictors) for use in model construction. This is important for the following reasons:\n",
    "\n",
    " i. **Simplicity:** Fewer features make the model simpler and easier to interpret.<br>\n",
    " ii. **Speed:** Less data means algorithms train faster.  <br>\n",
    " iii. **Prevention of overfitting:** Less redundant data means less opportunity to make decisions based on noise.<br>\n",
    "\n",
    "\n",
    " * **Feature Extraction**\n",
    "\n",
    "Feature extraction, on the other hand, is the process of transforming or mapping the original high-dimensional data into a lower-dimensional space. Unlike feature selection, where we keep the original features, feature extraction creates new ones that represent most of the \"useful\" information in the original data. The benefits are:\n",
    "\n",
    "**Dimensionality reduction:** Similar to feature selection, fewer features speed up training.<br>\n",
    "\n",
    "**Better performance:** Sometimes, the model can learn better in the transformed space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4OPxQeiE-2k"
   },
   "source": [
    "### Techniques for Feature Selection\n",
    "Feature selection methods are typically categorized into three classes: `filter methods`, `wrapper\n",
    "methods`, and `embedded methods`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2hMCsqhFK9s"
   },
   "source": [
    "* **Filter Methods**\n",
    "\n",
    "Filter methods select features based on their scores in statistical tests for their correlation with\n",
    "the outcome variable. Examples include the chi-squared test, information gain, and correlation coefficient scores. These methods are fast and straightforward but they ignore the potential combined effect of individual features.\n",
    "\n",
    " * **Wrapper Methods**\n",
    "\n",
    "Wrapper methods consider the selection of a set of features as a search problem, where\n",
    "different combinations are prepared, evaluated and compared to other combinations. A predictive model is used to evaluate a combination of features and assign a score based on model accuracy. Examples of wrapper methods are recursive feature elimination and forward\n",
    "selection. These methods often yield the best performance but can be very expensive computationally.\n",
    "\n",
    "* **Embedded Methods**\n",
    "\n",
    "Embedded methods learn which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods.\n",
    "\n",
    "Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (like a\n",
    "regression algorithm) that bias the model toward lower complexity (fewer coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TziBNOCeFUQx"
   },
   "outputs": [],
   "source": [
    "##FILTTER METHOD\n",
    "# Import libraries\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Feature selection\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "\n",
    "\n",
    "### Wrapper Methods\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "\n",
    "###Embedded Methods\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Build a regression dataset\n",
    "X, y = make_regression(noise=4, random_state=0)\n",
    "\n",
    "# LassoCV: Lasso linear model with iterative fitting along a regularization path\n",
    "lasso = LassoCV().fit(X, y)\n",
    "importance = np.abs(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OV28IFXQJtse"
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5OVOFmUIbwW"
   },
   "source": [
    "### Feature Extraction Methods\n",
    "\n",
    "Feature extraction methods reduce the dimensionality in the feature space by creating new\n",
    "features from the existing ones (and sometimes discarding the original features). Here are two\n",
    "widely-used techniques for feature extraction:\n",
    "\n",
    "* **Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA is a technique used to emphasize variation and bring out strong patterns in a dataset. It's\n",
    "often used to make data easy to explore and visualize.\n",
    "\n",
    "* **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "\n",
    "t-SNE is a machine learning algorithm for visualization developed by Laurens van der Maaten\n",
    "and Geoffrey Hinton. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions\n",
    "\n",
    "We shall examine code example for Feature Selection and Extraction. Now, let's put together the ideas discussed above into a real-world Python example but first, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxhlgCMoIuFn"
   },
   "outputs": [],
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "from sklearn.manifold import TSNE\n",
    "X_tsne = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJxuHsxQJ_TD"
   },
   "outputs": [],
   "source": [
    "## import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "## Next, we load the Iris dataset:\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "## Now we apply the filter method using chi-squared test:\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "\n",
    "## We apply the wrapper method using Support Vector Regression (SVR):\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "print(\"X shape after chi-squared feature selection: \", X_new.shape)\n",
    "\n",
    "# Let's use Recursive Feature Elimination (RFE) as a wrapper method:\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(\"X shape after RFE: \", X_new.shape)\n",
    "\n",
    "# Now let's try LassoCV as an embedded method:\n",
    "lasso = LassoCV().fit(X, y)\n",
    "importance = np.abs(lasso.coef_)\n",
    "idx_third = importance.argsort()[-3]\n",
    "threshold = importance[idx_third] + 0.01\n",
    "idx_features = (-importance).argsort()[:2]\n",
    "X_new = X[:, idx_features]\n",
    "print(\"X shape after LassoCV: \", X_new.shape)\n",
    "\n",
    "## Finally, we apply PCA and t-SNE for feature extraction:\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"X shape after PCA: \", X_pca.shape)\n",
    "X_tsne = TSNE(n_components=2).fit_transform(X)\n",
    "print(\"X shape after t-SNE: \", X_tsne.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhLrs7OEK0-t"
   },
   "source": [
    "### Wrap-up\n",
    "The importance of feature selection and extraction cannot be overstated. These techniques\n",
    "enable you to reduce the dimensionality of your data, which can both speed up the learning\n",
    "process and potentially increase your model's performance. Understanding these techniques is\n",
    "a vital part of the data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NLUVeyBLLqY"
   },
   "source": [
    "## Encoding Categorical Variables\n",
    "\n",
    "Categorical variables are a common type of non-numeric data variable that are critical in many\n",
    "data science and machine learning applications. Encoding categorical data is an important step\n",
    "in the data preprocessing stage. In this chapter, we'll examine what categorical variables are,\n",
    "their challenges, different encoding techniques, and how to handle high cardinality and rare\n",
    "categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHlS85t0LakS"
   },
   "source": [
    "### Understanding Categorical Variables and Their Challenges\n",
    "Categorical variables represent types of data which may be divided into groups. Examples of categorical variables are race, sex, age group, and educational level. While the latter two variables may also be continuous, they are often categorized in practice.\n",
    "\n",
    "Categorical variables pose a challenge when building machine learning models because these models, in essence, are algebraic. As a result, they require numerical inputs. This necessitates the transformation of categorical variables into a suitable numeric format, a process known as `categorical encoding`.\n",
    "\n",
    "However, not all encodings are suitable for every problem. The choice of encoding often depends on the specifics of the data and the model to be used. Furthermore, some encoding techniques can significantly increase the dimensionality of the dataset, leading to longer training times and a higher chance of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcEN1lTELv5L"
   },
   "source": [
    "### Techniques for Categorical Variable Encoding\n",
    "\n",
    "There are numerous techniques to encode categorical variables, each with its strengths and weaknesses. Here, we'll introduce two commonly used techniques: `one-hot encoding` and `label encoding`.\n",
    "\n",
    "* **One-hot encoding**\n",
    "\n",
    "One-hot encoding is a process of converting categorical data variables so they can be provided\n",
    "to machine learning algorithms to improve predictions. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0. Each integer value is represented as a binary vector.\n",
    "\n",
    "* **Label encoding**\n",
    "\n",
    "Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0G85hKfzMEDP"
   },
   "outputs": [],
   "source": [
    "## One-hot encoding\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `df` is your DataFrame and `category` is the categorical feature\n",
    "df_one_hot = pd.get_dummies(df, columns=['category'], prefix='category')\n",
    "\n",
    "## Label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['category_encoded'] = le.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrF_ldLmMcEs"
   },
   "source": [
    "### Dealing with High Cardinality and Rare Categories\n",
    "\n",
    "High cardinality means that a category feature has a lot of unique values, which can be problematic for certain encoding methods. For example, a one-hot encoding of a high cardinality feature can greatly expand the memory footprint of your dataset.\n",
    "\n",
    "One way to handle high cardinality is to group less common values into an **'other'** category. This can also help with the problem of rare categories, which may be present in your training data but unlikely to appear in future data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhUQdSSFMuQx"
   },
   "outputs": [],
   "source": [
    "##Dealing with High Cardinality and Rare Categories\n",
    "counts = df['category'].value_counts()\n",
    "other = counts[counts < threshold].index\n",
    "df['category'] = df['category'].replace(other, 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdvoMqA3M33X"
   },
   "source": [
    "### Python Code Examples for Categorical Variable Encoding\n",
    "\n",
    "Here's a full example of encoding a categorical feature in a dataset:\n",
    "\n",
    "In the code block below,\n",
    "* We start with a simple DataFrame containing name, sex, and city columns.\n",
    "\n",
    "* We then perform `one-hot encoding` on the sex column using the get_dummies function from pandas, and label encoding on the city column using `LabelEncoder` from `scikit-learn`. The result is a DataFrame where sex and city are converted into numeric formats suitable for a machine learning model.\n",
    "\n",
    "* Next, we address the issue of high cardinality and rare categories in the name column. We count the occurrence of each name using the `value_counts function`, and consider names that appear less than 2 times as `\"rare\"`. We then replace these rare names with the label `'Other'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fe-OFr50M84C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Let's create a simple DataFrame\n",
    "data = {'name': ['John', 'Lisa', 'Peter', 'Carla', 'Eva', 'John'],\n",
    "'sex': ['male', 'female', 'male', 'female', 'female', 'male'],\n",
    "'city': ['London', 'London', 'Paris', 'Berlin', 'Paris', 'Berlin']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encode the 'sex' column\n",
    "df_one_hot = pd.get_dummies(df, columns=['sex'], prefix='sex')\n",
    "\n",
    "# Label encode the 'city' column\n",
    "le = LabelEncoder()\n",
    "df['city_encoded'] = le.fit_transform(df['city'])\n",
    "\n",
    "# Display the original DataFrame and the modified DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nDataFrame after one-hot encoding 'sex' and label encoding 'city':\")\n",
    "print(df_one_hot)\n",
    "\n",
    "# Handle high cardinality and rare categories in 'name' column\n",
    "counts = df['name'].value_counts()\n",
    "\n",
    "# here we consider names appearing less than 2 times as \"rare\"\n",
    "other = counts[counts < 2].index\n",
    "df['name'] = df['name'].replace(other, 'Other')\n",
    "print(\"\\nDataFrame after handling high cardinality and rare categories in 'name'\n",
    "column:\")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5_ar0Y1NhAg"
   },
   "source": [
    "### Wrap-up\n",
    "Categorical encoding is a critical step in data preprocessing. Choosing the right encoding\n",
    "technique for your data and model can significantly impact the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bK1i0ziOLo_"
   },
   "source": [
    "## Handling Imbalanced Data\n",
    "Imbalanced datasets are a common problem in machine learning, where the number of observations in one class is significantly lower than the others. In this chapter, we will discuss what imbalanced data is, its impact on machine learning models, and various techniques for handling imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-jdkhSMOerh"
   },
   "source": [
    "### Understanding Imbalanced Data and Its Impact on Machine Learning\n",
    "\n",
    "Imbalanced data, as the name suggests, refers to a situation in classification problems where the classes are not represented equally. For example, in a binary classification problem, we may have 100 samples, with 90 samples belonging to class 'A' (the majority class) and only 10 samples belonging to class 'B' (the minority class). This is a classic scenario of an imbalanced dataset.\n",
    "\n",
    "The main problem with imbalanced datasets is that most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error. Thus, they tend to focus on\n",
    "the majority class and ignore the minority class. They might only predict the majority class, and hence have a high accuracy rate, but this isn't useful because the minority class, which is usually the point of interest, is completely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXmg_py3Oz3m"
   },
   "source": [
    "### Techniques for Handling Imbalanced Classes\n",
    "\n",
    "There are several strategies to handle imbalanced datasets. These strategies can broadly be divided into three categories: `resampling techniques`, `cost-sensitive learning`, and `ensemble methods`.\n",
    "\n",
    "* **Resampling Techniques**\n",
    "\n",
    "Resampling is the most straightforward way to handle imbalanced data, which involves removing samples from the majority class (undersampling) and/or adding more examples from the minority class (oversampling).\n",
    "\n",
    "* **Cost-Sensitive Learning**\n",
    "\n",
    "Cost-sensitive learning is a method that integrates the different misclassification costs (for false positives and false negatives) into the learning algorithm. In other words, it assigns higher costs to misclassifying minority class.\n",
    "\n",
    "\n",
    "* **Ensemble Methods**\n",
    "Ensemble methods, such as random forests or boosting algorithms, can also be used to deal with imbalanced datasets. These methods work by creating multiple models and then combining them to produce the final prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1eKIXPrPM-V"
   },
   "outputs": [],
   "source": [
    "#Resampling Techniques\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Assuming `X` is your feature set and `y` is the target variable\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "rus = RandomUnderSampler()\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "##Cost-Sensitive Learning\n",
    "from sklearn.svm import SVC\n",
    "# Create a SVC model with 'balanced' class weight\n",
    "clf = SVC(class_weight='balanced')\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "##Ensemble Methods\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a random forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4WoSOf-P_7i"
   },
   "source": [
    "### Code Example\n",
    "\n",
    "In this example, we used a popular oversampling technique called SMOTE (Synthetic Minority\n",
    "Over-sampling Technique). SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and drawing a new sample at a point along that line.\n",
    "\n",
    "Specifically, a random example from the minority class is first chosen. Then, k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen, and a synthetic example is created at a randomly selected point between the two examples in feature space.\n",
    "\n",
    "This approach is effective because new synthetic examples from the minority class are created that are plausible, that is, are relatively close in feature space to existing examples from the minority class.\n",
    "\n",
    "In the final section of the code, we created a Random Forest classifier, fit it to the resampled data, made predictions on the test set, and printed a\n",
    " classification report to observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkxH-2d2QAcZ"
   },
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Let's assume that we have a binary classification problem\n",
    "#with imbalanced classes\n",
    "\n",
    "data = pd.read_csv('data.csv') # Replace with your data file\n",
    "X = data.drop('target', axis=1) # Replace 'target' with your target variable\n",
    "y = data['target'] # Replace 'target' with your target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42, stratify=y)\n",
    "\n",
    "# Check the distribution of target variable\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Apply SMOTE to generate synthetic samples\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the distribution of target variable after applying SMOTE\n",
    "print(y_train_res.value_counts())\n",
    "\n",
    "# Create a random forest classifier and fit it to the resampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Predict on the test data and generate a classification report\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnUATF9JQ585"
   },
   "source": [
    "###Wrap-up\n",
    "as emonstrated, we provided an overview of the challenges and strategies related to handling\n",
    "imbalanced data. While we cover the most commonly used methods, it's worth noting that the optimal technique will depend on the specifics of the dataset and the problem at hand. Therefore, a good understanding of these methods is crucial for effectively handling\n",
    "imbalanced datasets and ultimately building robust and reliable machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D69XplbRU2X"
   },
   "source": [
    "## Data Integration and Transformation Techniques\n",
    "\n",
    "In the world of data science, working with clean, well-structured data is the exception, not the rule. Often, data is scattered across multiple sources, each with its own structure and format.\n",
    "\n",
    "Even when the data is all in one place, it might not be in a format that's optimal for the analysis or model you're planning to run. This chapter discusses data integration and transformation techniques that can help make the data more suitable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhs_MKQHRr5R"
   },
   "source": [
    "### Data Integration Approaches\n",
    "Data integration involves combining data from  different sources and providing users with a unified view of these data. This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their databases) and scientific (combining research findings from different bioinformatics repositories, for example) applications.\n",
    "\n",
    "* **Merging**\n",
    "\n",
    "Merging is the process of combining two or more data sets based on common columns between them.\n",
    "\n",
    "* **Joining**\n",
    "\n",
    "Joining is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame. In Pandas, we can join dataframes using the join function.\n",
    "\n",
    "* **Concatenating**\n",
    "\n",
    "Concatenation is a process of appending datasets, i.e., it adds dataframes along a particular\n",
    "axis, either row-wise or column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoznGx30SAov"
   },
   "outputs": [],
   "source": [
    "##Merging\n",
    "# Assuming `df1` and `df2` are your dataframes\n",
    "merged_df = pd.merge(df1, df2, on='common_column')\n",
    "\n",
    "##Joining\n",
    "# Assuming `df1` and `df2` are your dataframes\n",
    "joined_df = df1.join(df2, lsuffix='_df1', rsuffix='_df2')\n",
    "\n",
    "\n",
    "##Concatenating\n",
    "\n",
    "# Assuming `df1` and `df2` are your dataframes\n",
    "concat_df = pd.concat([df1, df2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN1NDcz3SiYu"
   },
   "source": [
    "### Data Transformation Techniques\n",
    "\n",
    "Data transformation is the process of converting data from one format or structure into another format or structure.\n",
    "\n",
    "\n",
    "* **Binning**\n",
    "\n",
    "Binning is a data transformation technique used to group a set of continuous values into bins\n",
    "or buckets. This can be particularly useful for managing noise or outliers.\n",
    "\n",
    "* **Log Transformation**\n",
    "\n",
    "Log transformation is a data transformation method in which it replaces each variable x with a log(x). The choice of the logarithm base is usually left up to the analyst and it would depend on the purposes of statistical modeling.\n",
    "\n",
    "* **Power Transformation**\n",
    "\n",
    "A power transformation is a statistical technique to make data more closely match a normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35usROVQSo9s"
   },
   "outputs": [],
   "source": [
    "## Binning\n",
    "\n",
    "# Assuming `df` is your dataframe and `age` is the column to bin\n",
    "bins = [0, 18, 35, 60, np.inf]\n",
    "names = ['<18', '18-35', '35-60', '60+']\n",
    "df['age_range'] = pd.cut(df['age'], bins, labels=names)\n",
    "\n",
    "##Log Transformation\n",
    "# Assuming `df` is your dataframe and `price` is the column to transform\n",
    "df['log_price'] = np.log(df['price'])\n",
    "\n",
    "\n",
    "##Power Transformation\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "# Assuming `X` is your feature set\n",
    "pt = PowerTransformer()\n",
    "X_transformed = pt.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdWzgDYeUdYU"
   },
   "source": [
    "###\n",
    "Handling Skewed Distributions and Nonlinear Relationships\n",
    "\n",
    "In statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. In other words, skewness tells you the amount and direction of skew (departure from horizontal symmetry). The skewness value can bepositive or negative, or undefined.\n",
    "\n",
    "To handle skewed data, we often use transformations like logarithm, square root, or cube root\n",
    "transformations which can normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-8xJTfdUnvk"
   },
   "outputs": [],
   "source": [
    "# Log transformation to handle right skewness\n",
    "# Assuming `df` is your dataframe and `income` is the skewed feature\n",
    "\n",
    "df['log_income'] = np.log(df['income'] + 1) # We add 1 to handle zero incomes\n",
    "\n",
    "# Confirming the change in skewness\n",
    "print(\"Old skewness: \", df['income'].skew())\n",
    "print(\"New skewness: \", df['log_income'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6ircWvWUqYm"
   },
   "source": [
    "Nonlinear relationships between variables can be addressed in several ways. One of the most common approaches is polynomial features, where features are raised to a power to capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJORYrJ2UvvT"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Assuming `X` is your feature set\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWEGUQOeUzHR"
   },
   "source": [
    "## Wrap-up\n",
    "We have provided a broad overview of data integration and transformation techniques that are essential in data preprocessing. Understanding these techniques is crucial, as real-world data often requires extensive cleaning, preprocessing, and transformation to reveal\n",
    "the underlying patterns and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJs8u0I-VP6i"
   },
   "source": [
    "## Case Study: Predicting House Prices\n",
    "\n",
    "In tis section, we will consolidate the various techniques discussed in the previous sections through a practical case study. By the end of this exercise, we should have a firm grasp of how to apply data cleaning and preprocessing techniques in a real-world context.\n",
    "\n",
    "For our case study, we will work with the [Ames Housing](https://github.com/OluwaMarg/Introduction-to-Python-for-Data-Analytics/blob/main/WK5_DATA%20CLEANING%202/AmesHousing.csv) dataset, a richly detailed and relatively large dataset with 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. Our task will be to predict the final price of each home.\n",
    "\n",
    "The first step, as always, is to **load our data and the necessary Python libraries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yS_knJuY1oj"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdWM5wbEWFQG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('AmesHousing.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jddmb-eiZL_h"
   },
   "source": [
    "We'll then **split our data into training and test sets**. It's important to **conduct preprocessing** steps separately on these sets to avoid data leakage, which can lead to overly optimistic performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EALX27IGZS29"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btSXIgA2ZYWb"
   },
   "source": [
    "Now, let's take a look at the first few rows of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrmRV4LkZX_y"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODjC3AttZjP6"
   },
   "source": [
    "Given the number of features in this dataset, we can expect a variety of data cleaning and\n",
    "preprocessing tasks. We will need to **handle missing data, outliers, categorical variables**, and\n",
    "possibly more.\n",
    "\n",
    "Let's start with **missing data**.This will show us the count of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us4ph6LzZjpY"
   },
   "outputs": [],
   "source": [
    "# Checking for missing data\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "print(missing_values.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzmtDnxLZtwH"
   },
   "source": [
    "For simplicity, let's impute missing values with the median for numerical features, and the most frequent value for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qylCarKUZ652"
   },
   "outputs": [],
   "source": [
    "Create our imputers\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "# Get lists of numeric and categorical column names\n",
    "num_cols = train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_cols = train_df.select_dtypes(include='object').columns.tolist()\n",
    "# Impute missing values\n",
    "train_df[num_cols] = num_imputer.fit_transform(train_df[num_cols])\n",
    "train_df[cat_cols] = cat_imputer.fit_transform(train_df[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZdvhpkHZ6cO"
   },
   "source": [
    "Next, let's handle **outliers**. For simplicity, we'll use the IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DhXn2nPaET6"
   },
   "outputs": [],
   "source": [
    "Q1 = train_df[num_cols].quantile(0.25)\n",
    "Q3 = train_df[num_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Removing outliers\n",
    "train_df = train_df[~((train_df < (Q1 - 1.5 * IQR)) | (train_df > (Q3 + 1.5 *\n",
    "IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZigwQLxBaIRk"
   },
   "source": [
    "For **encoding categorical variables**, we'll use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg-0JkA1aMIw"
   },
   "outputs": [],
   "source": [
    "# Create a one-hot encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Apply the encoder to the categorical columns\n",
    "train_df_encoded = pd.DataFrame(encoder.fit_transform(train_df[cat_cols]))\n",
    "\n",
    "# Add back the index and column names\n",
    "train_df_encoded.index = train_df.index\n",
    "train_df_encoded.columns = encoder.get_feature_names(input_features=cat_cols)\n",
    "\n",
    "# Drop the original categorical columns and replace with the encoded ones\n",
    "train_df = train_df.drop(cat_cols, axis=1)\n",
    "train_df = pd.concat([train_df, train_df_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIhx9SVqaach"
   },
   "source": [
    "With categorical variables handled, we can now move on to scaling the data. For this, we'll use the **StandardScaler** from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kU-aMZk8c1Sv"
   },
   "outputs": [],
   "source": [
    "#Create a standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the numeric columns\n",
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m3X4qmOc2Q-"
   },
   "source": [
    "Finally, we need to address the **issue of imbalanced data**. This is a regression task, so we won't\n",
    "need to worry about imbalanced classes. However, in a classification task, we might use techniques such as `resampling`, `cost-sensitive learning`, or `ensemble methods` to handle imbalanced classes\n",
    "\n",
    "\n",
    "Now that we've preprocessed our training data, we can **apply the same transformations to the validation and test sets**. Note that we're using transform, not fit_transform, to ensure that the same transformations are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "farOuGafdViy"
   },
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "val_df[num_cols] = num_imputer.transform(val_df[num_cols])\n",
    "val_df[cat_cols] = cat_imputer.transform(val_df[cat_cols])\n",
    "\n",
    "# Remove outliers (note: this is a simplified example)\n",
    "val_df = val_df[~((val_df < (Q1 - 1.5 * IQR)) | (val_df > (Q3 + 1.5 *\n",
    "IQR))).any(axis=1)]\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "val_df_encoded = pd.DataFrame(encoder.transform(val_df[cat_cols]))\n",
    "val_df_encoded.index = val_df.index\n",
    "val_df_encoded.columns = encoder.get_feature_names(input_features=cat_cols)\n",
    "val_df = val_df.drop(cat_cols, axis=1)\n",
    "val_df = pd.concat([val_df, val_df_encoded], axis=1)\n",
    "\n",
    "# Scale numeric columns\n",
    "val_df[num_cols] = scaler.transform(val_df[num_cols])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
